<div align="center">
  <img src="docs/img/logo.png" alt="Logo da UFAL" width="200"/>
</div>

<div align="center">

### Universidade Federal de Alagoas (UFAL)
### Instituto de Computa√ß√£o  
#### Programa de P√≥s-Gradua√ß√£o em Inform√°tica  

**Projeto de Mestrado**  

</div>

Reposit√≥rio para armazenar estudos, projetos e materiais relacionados ao Mestrado em Inform√°tica na Universidade Federal de Alagoas (UFAL). Inclui c√≥digos-fonte, documentos, apresenta√ß√µes e outros recursos desenvolvidos durante o curso.




## üìå **T√≠tulo**  
**Detec√ß√£o Avan√ßada de M√≠dias Sint√©ticas em V√≠deos mediante An√°lise de Complexidade-Entropia**  

üë®‚Äçüéì **Aluno:** F√°bio Linhares  
üë©‚Äçüè´ **Orientadora:** Prof.¬™ Dr.¬™ Fabiane da Silva Queiroz  
üî¨ **Linha de Pesquisa:** Computa√ß√£o Visual e Inteligente  
üéØ **Tema de Pesquisa:** Vis√£o Computacional: An√°lise, Caracteriza√ß√£o e Classifica√ß√£o de Padr√µes Din√¢micos e Estruturais em M√≠dias Sint√©ticas

---

[‚û°Ô∏è Relat√≥rio executivo da RSL](docs/picoc/qa/RELATORIO_EXECUTIVO.md)

---

## üìù **Introdu√ß√£o**  

A prolifera√ß√£o de **m√≠dias sint√©ticas**, popularmente conhecidas como deepfakes, representa um desafio crescente para a seguran√ßa da informa√ß√£o e a confian√ßa no ecossistema digital. A r√°pida evolu√ß√£o dos modelos generativos, como **Redes Adversariais Generativas (GANs)** e **Modelos de Difus√£o**, torna os m√©todos de detec√ß√£o baseados em artefatos espec√≠ficos rapidamente obsoletos. A comunidade de pesquisa enfrenta a necessidade premente de desenvolver detectores que n√£o apenas apresentem alta acur√°cia, mas que tamb√©m generalizem para m√©todos de manipula√ß√£o desconhecidos e n√£o vistos durante o treinamento.

A literatura atual √© dominada por abordagens de aprendizado profundo, como **Redes Neurais Convolucionais (CNNs)** e **Vision Transformers (ViTs)**, que, apesar de seu desempenho not√°vel, frequentemente operam como "caixas-pretas". Esses modelos podem aprender correla√ß√µes esp√∫rias nos dados de treinamento, o que limita sua robustez em cen√°rios do mundo real. Existe uma lacuna significativa na literatura no que tange a m√©todos de detec√ß√£o fundamentados em princ√≠pios te√≥ricos que explorem a natureza intr√≠nseca do conte√∫do gerado por IA.

### **üéØ Mudan√ßa de Paradigma Proposta**

Este projeto de pesquisa prop√µe uma **mudan√ßa de paradigma**. Em vez de tratar imagens geradas por IA como imagens aut√™nticas com defeitos, hipotetizamos que elas s√£o o produto de um **sistema din√¢mico complexo e determin√≠stico**. Argumentamos que tais sistemas imprimem uma **"textura estat√≠stica"** √∫nica e mensur√°vel, caracterizada por uma assinatura espec√≠fica no espa√ßo de complexidade-entropia, an√°loga √† de sistemas ca√≥ticos.

Propomos o **Plano Causalidade Entropia-Complexidade (Plano CH)** como a ferramenta principal para capturar essa assinatura fundamental, visando criar um detector que seja, por constru√ß√£o, mais generaliz√°vel e interpret√°vel. Esta abordagem combina a robustez te√≥rica da **Teoria da Informa√ß√£o** com a capacidade de representa√ß√£o dos modelos de **aprendizado profundo**, oferecendo uma solu√ß√£o h√≠brida e inovadora para o problema da detec√ß√£o de m√≠dias sint√©ticas.

---

## üìö **Justificativa**

A era da informa√ß√£o digital √© marcada por um fluxo massivo de conte√∫do cuja veracidade √© frequentemente questionada. Imagens e v√≠deos n√£o naturais ‚Äî ou seja, gerados parcial ou totalmente por algoritmos de intelig√™ncia artificial, contendo um ou mais rostos humanos trocados ou n√£o ‚Äî constituem um novo tipo de artefato comunicacional: o que chamaremos **produtos de IA**.

A populariza√ß√£o de algoritmos generativos, como as **Redes Adversariais Generativas (GANs)** e os **modelos de difus√£o**, tem permitido a cria√ß√£o de conte√∫do sint√©tico visualmente consistente, muitas vezes indistingu√≠vel, a olho nu, de conte√∫do natural e aut√™ntico. Isso levanta s√©rias preocupa√ß√µes sobre **desinforma√ß√£o**, **manipula√ß√£o de opini√£o p√∫blica** e **danos √† imagem pessoal e coletiva**.

### **Limita√ß√µes das Abordagens Atuais**

Pesquisas voltadas √† detec√ß√£o desses produtos sint√©ticos concentradas, em grande parte, em abordagens baseadas em **Deep Learning (DL)**, como Redes Neurais Convolucionais (CNNs) e Vision Transformers (ViTs) t√™m demonstrado resultados promissores. No entanto, muitos desses m√©todos focam na an√°lise de artefatos espaciais e na detec√ß√£o de anomalias em quadros individuais.

A **natureza temporal dos v√≠deos**, onde a evolu√ß√£o dos padr√µes e correla√ß√µes ao longo do tempo √© crucial, nos parece menos explorada. Produtos de IA em v√≠deo frequentemente carregam **tra√ßos din√¢micos at√≠picos**, exibem **inconsist√™ncias temporais sutis**, como falhas em padr√µes de piscar, movimentos de cabe√ßa n√£o naturais, ou transi√ß√µes abruptas entre express√µes faciais, que podem n√£o ser evidentes em um √∫nico quadro, mas se tornam detect√°veis ao analisar a s√©rie temporal de caracter√≠sticas extra√≠das.

### **Fundamenta√ß√£o Te√≥rica**

√â neste ponto que as ferramentas da **Teoria da Informa√ß√£o** e da **An√°lise de Sistemas Din√¢micos Complexos** se mostram particularmente adequadas. A **entropia de Shannon** quantifica a incerteza de um sistema, enquanto a **complexidade estat√≠stica** mede o grau de estrutura e padr√µes, complementando a entropia.

O **Plano Complexidade-Entropia (CECP)**, e sua extens√£o **Multivariada (MvCECP)**, provaram ser eficazes na distin√ß√£o de sistemas com din√¢micas variadas ‚Äî peri√≥dicas, ca√≥ticas e estoc√°sticas ‚Äî ao mapear as caracter√≠sticas de suas s√©ries temporais em um espa√ßo bidimensional.

A **entropia de permuta√ß√£o** (Bandt e Pompe) √© uma medida robusta e computacionalmente eficiente para extrair padr√µes ordinais de s√©ries temporais. O par√¢metro **embedding delay (œÑ)**, por sua vez, permite investigar as s√©ries temporais em diferentes escalas de tempo, revelando din√¢micas ocultas ou an√¥malas.

### **Potencial de Detec√ß√£o**

Acreditamos que a aplica√ß√£o dessas ferramentas aos produtos de IA permitir√° capturar as **"digitais" din√¢micas da manipula√ß√£o** de forma mais precisa. Por exemplo, a suavidade excessiva de certas √°reas manipuladas ou a aus√™ncia de padr√µes ordinais esperados em movimentos faciais podem ser detectadas como desvios em medidas de complexidade-entropia. Al√©m disso, a Teoria da Estima√ß√£o Estat√≠stica, particularmente o princ√≠pio da m√°xima entropia de Jaynes, fornecer√° a base formal para inferir as distribui√ß√µes de probabilidade que melhor representam os dados, garantindo que as infer√™ncias sobre a natureza das m√≠dias sint√©ticas sejam as menos preconceituosas e mais objetivas poss√≠veis.

---

## üéØ **Objetivos do Projeto**  

### **üîπ Objetivo Geral**
Desenvolver e validar um **framework h√≠brido e generaliz√°vel** para a detec√ß√£o de v√≠deos deepfake, fundamentado na sinergia entre a an√°lise de complexidade estat√≠stica e a extra√ß√£o de features de aprendizado profundo.

### **üîπ Objetivos Espec√≠ficos**  
1. **Pipeline de Extra√ß√£o:** Implementar um pipeline robusto para a extra√ß√£o das coordenadas (H,C) do Plano CH a partir de frames de v√≠deo, incluindo uma an√°lise de sensibilidade aos par√¢metros de embedding dx e dy.

2. **Mapeamento de Assinaturas:** Mapear e caracterizar as "assinaturas de complexidade" de v√≠deos reais e falsos de m√∫ltiplos datasets (e.g., FaceForensics++, Celeb-DF) no Plano CH, validando empiricamente a Hip√≥tese de Separa√ß√£o.

3. **An√°lise de Robustez:** Avaliar a robustez das features (H,C) a perturba√ß√µes comuns do mundo real, como compress√£o de v√≠deo, adi√ß√£o de ru√≠do e varia√ß√µes de ilumina√ß√£o.

4. **Modelo H√≠brido:** Construir, treinar e validar um modelo h√≠brido que combine F_CH e F_ViT, testando sua capacidade de generaliza√ß√£o contra um modelo baseline.

5. **Interpretabilidade:** Oferecer explica√ß√µes e insights sobre os mecanismos de detec√ß√£o, interpretando como as medidas capturam as anomalias.

---

## üî¨ **Hip√≥teses de Pesquisa**

### **H1 (Hip√≥tese de Separa√ß√£o):**
Imagens geradas por diferentes modelos de IA (e.g., GANs, Modelos de Difus√£o) e imagens aut√™nticas ocupar√£o regi√µes estatisticamente separ√°veis no Plano Causalidade Entropia-Complexidade.

### **H2 (Hip√≥tese de Efici√™ncia Informacional):**
O vetor de features bidimensional F_CH=[H,C], derivado do Plano CH, constitui um estimador estatisticamente mais eficiente da classe da imagem (real vs. falsa) do que features baseadas em artefatos, como as derivadas da An√°lise de N√≠vel de Erro (ELA).

### **H3 (Hip√≥tese de Sinergia H√≠brida):**
Um modelo de classifica√ß√£o que funde as features interpret√°veis do Plano CH (F_CH) com as features de representa√ß√£o global aprendidas por um Vision Transformer (F_ViT) exibir√° desempenho superior em acur√°cia e generaliza√ß√£o.

---

## üõ† **Metodologia Proposta**

### **1Ô∏è‚É£ Pipeline de Extra√ß√£o de Features Estat√≠sticas (F_CH)**
- **Implementa√ß√£o:** Convers√£o de frames para escala de cinza e varredura por janela deslizante de tamanho dx√ódy
- **Par√¢metros:** Investiga√ß√£o de dimens√µes de embedding dx e dy (e.g., 2√ó2, 3√ó2) respeitando (dx‚ãÖdy)!‚â™W‚ãÖH
- **Sa√≠da:** Vetor [H,C] para cada frame, constituindo features de baixa dimens√£o, computacionalmente eficientes e interpret√°veis

### **2Ô∏è‚É£ Pipeline de Extra√ß√£o de Features de Deep Learning (F_ViT)**
- **Arquitetura:** Vision Transformer (ViT) pr√©-treinado (ViT-Base/16) como extrator "congelado"
- **Extra√ß√£o:** Vetor de embedding do token `[CLS]` da √∫ltima camada para formar F_ViT
- **Justificativa:** Complementaridade conceitual entre padr√µes ordinais locais (PE2D) e depend√™ncias globais (ViT)

### **3Ô∏è‚É£ Fus√£o de Features e Classifica√ß√£o**
- **M√©todo:** Concatena√ß√£o simples: F_hybrid = [F_CH, F_ViT]
- **Classificador:** Gradient Boosting (XGBoost/LightGBM) para dados tabulares heterog√™neos
- **Baseline:** Modelo utilizando apenas F_ViT para valida√ß√£o da Hip√≥tese de Sinergia

### **4Ô∏è‚É£ Datasets Experimentais**


A seguir est√£o os datasets utilizados neste trabalho. Para cada um fornecemos uma breve descri√ß√£o, origem/identificador e observa√ß√µes relevantes para reprodutibilidade e conformidade.


#### AI Generated Images - High Quality  
- **Descri√ß√£o breve:** Conjunto de imagens de alta qualidade geradas por modelos de s√≠ntese de imagens (GANs / diffusion models). Focado em rostos e cenas realistas produzidas por IA; √∫til para treinar e avaliar classificadores que discriminam imagens sint√©ticas de imagens reais.  
- **Origem / identificador:** Kaggle ‚Äî `shahzaibshazoo/detect-ai-generated-faces-high-quality-dataset`  
- **Conte√∫do t√≠pico:** Imagens geradas por IA em alta resolu√ß√£o, agrupadas por fonte/gerador quando dispon√≠vel; metadados podem incluir r√≥tulos de origem do gerador.  
- **Uso neste projeto:** Base para avalia√ß√£o da capacidade do classificador em identificar artefatos de imagens sintetizadas e medir robustez frente a alta qualidade visual.  


---

#### Deepfake and Real Images  
- **Descri√ß√£o breve:** Conjunto misto contendo imagens reais e imagens deepfake (manipuladas ou sintetizadas) ‚Äî indicado para tarefas de classifica√ß√£o bin√°ria (real vs. fake) e experimentos de generaliza√ß√£o.  
- **Origem / identificador:** Kaggle ‚Äî `manjuts98/deepfake-and-real-images`  
- **Conte√∫do t√≠pico:** Pares ou cole√ß√µes de imagens reais e suas respectivas manipula√ß√µes/deepfakes; pode conter subpastas por classe (real / fake) e metadados sobre m√©todo de s√≠ntese/forgery.  
- **Uso neste projeto:** Treino e valida√ß√£o de modelos para detec√ß√£o de deepfakes com foco em desempenho entre diferentes fontes/fornecedores de manipula√ß√£o.  

---

#### **üéØ Bases de Dados Utilizadas (at√© agora)**

##### Web of Science - Cole√ß√£o Principal (Clarivate Analytics / Thomson Reuters)
- Provedor: Clarivate Analytics
- Resumo: Base multidisciplinar que indexa somente os peri√≥dicos mais citados em suas respectivas √°reas. √â tamb√©m um √≠ndice de cita√ß√µes, informando, para cada artigo, os documentos por ele citados e os documentos que o citaram. Possui hoje mais de 9.000 peri√≥dicos indexados. √â composta por:
  - Science Citation Index Expanded (SCI-EXPANDED): 1945 at√© o presente
  - Social Sciences Citation Index: 1956 at√© o presente
  - Arts and Humanities Citation Index: 1975 at√© o presente
  - A partir de 2012 o conte√∫do foi ampliado com a inclus√£o do Conference Proceedings Citation Index - Science (CPCI-S) e Conference Proceedings Citation Index - Social Science & Humanities (CPCI-SSH).

---

##### IEEE Xplore Digital Library
- Provedor: Institute of Electrical and Electronic Engineers Incorporated (IEEE)
- Resumo: O Institute of Electrical and Electronic Engineers Incorporated (IEEE) √© uma organiza√ß√£o dedicada ao avan√ßo da inova√ß√£o e da excel√™ncia tecnol√≥gica para o benef√≠cio da humanidade que foi projetada para atender profissionais envolvidos em todos os aspectos dos campos el√©trico, eletr√¥nico, e de computa√ß√£o e demais √°reas afins da ci√™ncia e tecnologia que fundamentam a civiliza√ß√£o moderna. Criado em 1884, nos E.U.A., o IEEE congrega mais de 410.000 associados, entre engenheiros, cientistas, pesquisadores e outros profissionais, em cerca de 160 pa√≠ses. √â composto por um Conselho de Diretores e por um Comit√™ Executivo que compreende 10 Regi√µes, 39 Sociedades T√©cnicas, 7 Conselhos T√©cnicos e aproximadamente 1850 Comit√™s Societ√°rios e 342 Se√ß√µes. A cole√ß√£o IEEE Xplore Digital Library inclui texto completo desde 1988. Oferece publica√ß√µes de peri√≥dicos, normas t√©cnicas e revistas em engenharia el√©trica, computa√ß√£o, biotecnologia, telecomunica√ß√µes, energia e dezenas de outras tecnologias. Al√©m disso, o IEEE fornece acesso a mais de 6 milh√µes de documentos, incluindo artigos de pesquisa, normas t√©cnicas, anais de congressos, tabelas e gr√°ficos, conjunto de dados, artigos de transa√ß√µes internacionais, e-books, publica√ß√µes de confer√™ncias, patentes e peri√≥dicos. Possui ainda a cole√ß√£o IEEE Access, que √© um peri√≥dico multidisciplinar, somente online, de acesso totalmente aberto (acesso gold), apresentando continuamente os resultados de pesquisa original ou desenvolvimento em todos os campos de interesse do IEEE. Apoiado por Taxa de Processamento de Artigo ‚Äì APC, seus artigos s√£o revisados por pares, a submiss√£o para publica√ß√£o √© de 4 a 6 semanas e os artigos ficam dispon√≠veis gratuitamente para todos os leitores. Possui fator de impacto pr√≥prio, pontos de influ√™ncia do artigo e CiteScore estimados. Al√©m de normas e proceedings, atualmente o Portal de Peri√≥dicos assina 239 per√≠odicos para leitura e financia a publica√ß√£o em 217 t√≠tulos disponibilizados pela IEEE, de modo que 163 institui√ß√µes t√™m sido beneficiadas. O conte√∫do atende as seguintes grandes √°reas da tabela CAPES: Ci√™ncias Exatas e da Terra e Engenharias.

---

##### SCOPUS (Elsevier)
- Provedor: Reed Elsevier
- Resumo: Scopus is a comprehensive scientific, medical, technical and social science database containing all relevant literature.

---

##### ScienceDirect (Elsevier)
- Provedor: Elsevier
- Resumo: A ScienceDirect cont√©m artigos de mais de 3.800 di√°rios e mais de 37.000 t√≠tulos de livros, muitas de suas publica√ß√µes s√£o aprimoradas com elementos interativos fornecidos por autores, como √°udio, v√≠deo, gr√°ficos, tabelas e imagens. Os artigos tamb√©m possuem links incorporados para conjuntos de dados externos, como Scopus¬Æ, PANGEA¬Æ e Reaxys¬Æ. Combinando esses extras de conte√∫do com o texto de cada artigo e se obter√° uma compreens√£o completa do panorama da informa√ß√£o antes de avan√ßar seu trabalho. Est√£o dispon√≠veis publica√ß√µes cobrindo as √°reas de Ci√™ncias Biol√≥gicas, Ci√™ncias da Sa√∫de, Ci√™ncias Agr√°rias, Ci√™ncias Exatas e da Terra, Engenharias, Ci√™ncias Sociais Aplicadas, Ci√™ncias Humanas e Letras e Artes.

---

## üìö **Base Te√≥rica Fundamental**

#### **üìÑ Complexity-entropy causality plane as a complexity measure for two-dimensional patterns**
- **Autores:** Ribeiro, H. V.; Zunino, L.; Lenzi, E. K.; Santoro, P. A.; Mendes, R. S.
- **Ano:** 2012
- **Contribui√ß√£o:** Fundamenta√ß√£o te√≥rica do Plano CH para an√°lise bidimensional de padr√µes
- **Aplica√ß√£o:** Base matem√°tica para extra√ß√£o de features F_CH

#### **üìÑ Distinguishing noise from chaos**
- **Autores:** Schreiber, T.; Schmitz, A.
- **Ano:** 1996
- **Contribui√ß√£o:** Metodologia para separa√ß√£o de din√¢micas determin√≠sticas e estoc√°sticas
- **Aplica√ß√£o:** Valida√ß√£o da Hip√≥tese de Separa√ß√£o (H1)

#### **üìÑ Information Theory and Statistical Mechanics**
- **Autor:** Jaynes, E. T.
- **Ano:** 1957
- **Contribui√ß√£o:** Princ√≠pio da m√°xima entropia para infer√™ncia estat√≠stica
- **Aplica√ß√£o:** Infer√™ncia estat√≠stica objetiva sobre m√≠dias sint√©ticas

#### **üìÑ How to conduct a systematic literature review: A narrative guide**
- **Autores:** Mengist, W.; Soromessa, T.; Legese, G.
- **Ano:** 2020
- **Contribui√ß√£o:** Metodologia PICOC para revis√£o sistem√°tica
- **Aplica√ß√£o:** Estrutura√ß√£o da pesquisa bibliogr√°fica

## üîç **PICOC: Implementa√ß√£o e Resultados**

Para estruturar sistematicamente a revis√£o da literatura, utilizaremos o protocolo **PICOC (Population, Intervention, Comparison, Outcomes, Context)**, que fornece um framework robusto para a formula√ß√£o de quest√µes de pesquisa e busca bibliogr√°fica:

### **üéØ Population/Problem (Problema)**
- **Imagens e v√≠deos digitais** gerados por algoritmos de intelig√™ncia artificial
- **M√≠dias sint√©ticas** criadas ou alteradas por GANs, modelos de difus√£o e outras t√©cnicas generativas
- **Datasets de refer√™ncia**: FaceForensics++, Celeb-DF, DFDC, etc.

### **üî¨ Intervention (Interven√ß√£o)**
- **An√°lise de complexidade-entropia** baseada em entropia de permuta√ß√£o
- **Plano Causalidade Entropia-Complexidade (Plano CH)**
- **Extra√ß√£o de features estat√≠sticas** usando padr√µes ordinais bidimensionais
- **Fus√£o com features de Vision Transformers** para detec√ß√£o h√≠brida

### **‚öñÔ∏è Comparison (Compara√ß√£o)**
- **M√©todos tradicionais** baseados em CNNs (ResNet, EfficientNet)
- **Abordagens de an√°lise de artefatos** (ELA, an√°lise espectral)
- **Detectores baseados em ViTs** puros
- **M√©todos ensemble** convencionais

### **üìä Outcomes (Resultados)**
- **Acur√°cia de detec√ß√£o** (AUC-ROC, EER)
- **Capacidade de generaliza√ß√£o** cross-dataset
- **Robustez** a perturba√ß√µes (compress√£o, ru√≠do)
- **Interpretabilidade** dos mecanismos de detec√ß√£o
- **Efici√™ncia computacional**

### **üåç Context (Contexto)**
- **Detec√ß√£o de deepfakes** em ambiente controlado e real
- **Aplica√ß√µes de seguran√ßa da informa√ß√£o**
- **Cen√°rios de forense digital**
- **Mitiga√ß√£o de desinforma√ß√£o**

---
#### **üéØ Bases de Dados Utilizada (at√© agora)**
- **Web of Science:** Cole√ß√£o Principal (1945-presente) - 9.000+ peri√≥dicos indexados
- **IEEE Xplore:** Biblioteca Digital completa (1988-presente) - 6M+ documentos
- **Scopus (Elsevier):** Base multidisciplinar abrangente
- **ScienceDirect:** 3.800+ peri√≥dicos e 37.000+ t√≠tulos de livros

---



#### **üìÑ Artigos Selecionados (Primeira avalia√ß√£o em andamento)**

**Surveys e Reviews Fundamentais**

* Ahmed S.R. (2022): "Analysis Survey on Deepfake detection and Recognition with Convolutional Neural Networks" ‚Äî *Hora 2022 (4th Int. Congress on Human Computer Interaction Optimization and Robotic Applications Proceedings)*
* Chennamma H.R. (2023): "A comprehensive survey on image authentication for tamper detection with localization" ‚Äî *Multimedia Tools and Applications*
* Kadha V. (2025): "Unravelling Digital Forgeries: A Systematic Survey on Image Manipulation Detection and Localization" ‚Äî *ACM Computing Surveys*
* Khan A.A. (2025): "A survey on multimedia-enabled deepfake detection: state-of-the-art tools and techniques, emerging trends, current challenges & limitations, and future directions" ‚Äî *Discover Computing*
* Li C. (2025): "Survey on Technologies of Video Deepfake Detection" ‚Äî *Lecture Notes on Data Engineering and Communications Technologies*
* Liang R. (2020): "A Survey of Audiovisual Deepfake Detection Techniques" ‚Äî *Journal of Cyber Security*
* Nguyen T.T. (2022): "Deep learning for deepfakes creation and detection: A survey" ‚Äî *Computer Vision and Image Understanding*
* Rana M.S. (2022): "Deepfake Detection: A Systematic Literature Review" ‚Äî *IEEE Access*

**M√©todos de An√°lise Temporal / Landmark & Motion**

* Liao X. (2023): "FAMM: Facial Muscle Motions for Detecting Compressed Deepfake Videos Over Social Networks" ‚Äî *IEEE Transactions on Circuits and Systems for Video Technology*
* Sornavalli G. (2024): "DeepFake Detection by Prediction of Mismatch Between Audio and Video Lip Movement" ‚Äî *ADICS 2024 (International Conference on Advances in Data Engineering and Intelligent Computing Systems)*
* Sharma H. (2021): "Video interframe forgery detection: Classification, technique & new dataset" ‚Äî *Journal of Computer Security*
* Zhang Y. (2025): "Exploring coordinated motion patterns of facial landmarks for deepfake video detection" ‚Äî *Applied Soft Computing*
* Zhu C. (2024): "Deepfake detection via inter-frame inconsistency recomposition and enhancement" ‚Äî *Pattern Recognition*

**Abordagens de An√°lise de Frequ√™ncia / Espacial‚ÄìFrequencial**

* Frank J.; Eisenhofer T.; Sch√∂nherr L. (2020): "Leveraging Frequency Analysis for Deep Fake Image Recognition" ‚Äî *ICML / PMLR* (refer√™ncia cl√°ssica √∫til)
* Qiusong L. (2025): "Joint spatial-frequency deepfake detection network based on dual-domain attention-enhanced deformable convolution" ‚Äî *Applied Intelligence*
* Shi Z. (2025): "Customized Transformer Adapter With Frequency Masking for Deepfake Detection" ‚Äî *IEEE Transactions on Information Forensics and Security*

**M√©todos Baseados em Teoria da Informa√ß√£o & Entropia**

* Sheng Z. (2025): "SUMI-IFL: An Information-Theoretic Framework for Image Forgery Localization with Sufficiency and Minimality Constraints" ‚Äî *Proceedings of the AAAI Conference on Artificial Intelligence*
* Sudarsan M. (2025): "LEAD-AI: Lightweight Entropy Analysis for Distinguishing AI-Generated Images from Genuine Photographs" ‚Äî *Proceedings of SPIE (The International Society for Optical Engineering)*
* Sun K. (2022): "An Information Theoretic Approach for Attention-Driven Face Forgery Detection" ‚Äî *Lecture Notes in Computer Science*

**Arquiteturas Transformer / Vision Transformers / Adapters**

* Atamna M. (2025): "WaveConViT: Wavelet-Based Convolutional Vision Transformer for Cross-Manipulation Deepfake Video Detection" ‚Äî *Lecture Notes in Computer Science*
* D. Zhang (2025): "DPL: Cross-Quality DeepFake Detection via Dual Progressive Learning" ‚Äî *Lecture Notes in Computer Science*
* Li S. (2024): "UnionFormer: Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization" ‚Äî *Proceedings of CVPR (IEEE Computer Society Conference on Computer Vision and Pattern Recognition)*

**Robustez, Generaliza√ß√£o e Continual Learning**

* Bai N. (2025): "Towards generalizable face forgery detection via mitigating spurious correlation" ‚Äî *Neural Networks*
* Sun K. (2025): "Continual Face Forgery Detection via Historical Distribution Preserving" ‚Äî *International Journal of Computer Vision*
* Xu K. (2024): "RLGC: Reconstruction Learning Fusing Gradient and Content Features for Efficient Deepfake Detection" ‚Äî *IEEE Transactions on Consumer Electronics*

**Localiza√ß√£o de Forgeries / Detec√ß√£o Forense e Otimiza√ß√£o**

* Chen J. (2023): "Identification of image global processing operator chain based on feature decoupling" ‚Äî *Information Sciences*
* Iseed S.Y. (2023): "Forensic approach for distinguishing between source and destination regions in copy-move forgery" ‚Äî *Multimedia Tools and Applications*
* Joshi D. (2025): "Optimized detection and localization of copy-rotate-move forgeries using biogeography-based optimization algorithm" ‚Äî *Journal of Forensic Sciences*
* Peng C. (2025): "Within 3DMM Space: Exploring Inherent 3D Artifact for Video Forgery Detection" ‚Äî *IEEE Transactions on Information Forensics and Security*

**Modelos Leves, Codec Seguro & Consumer Electronics**

* Huang C.H. (2025): "A Secure Learned Image Codec for Authenticity Verification via Self-Destructive Compression" ‚Äî *Big Data and Cognitive Computing*
* Jin Z. (2025): "Protecting Consumer Electronics Human-Computer Interactive Verification Security via Anomaly-Aware Reconstruction-Guided Forgery Localization" ‚Äî *IEEE Transactions on Consumer Electronics*
* Sudarsan M. (2025): "LEAD-AI: Lightweight Entropy Analysis for Distinguishing AI-Generated Images from Genuine Photographs" ‚Äî *Proceedings of SPIE*

**Multimodalidade (√Åudio‚ÄìV√≠deo / Multimodal)**

* Das A.K. (2023): "A Multi-stage Multi-modal Classification Model for DeepFakes Combining Deep Learned and Computer Vision Oriented Features" ‚Äî *Lecture Notes in Computer Science*
* Liu B. (2022): "Detecting Generated Images by Real Images" ‚Äî *Lecture Notes in Computer Science*
* Sornavalli G. (2024): "DeepFake Detection by Prediction of Mismatch Between Audio and Video Lip Movement" ‚Äî *ADICS 2024*

**Detec√ß√£o baseada em Difus√£o / Latent Diffusion & Flags**

* Ricker J. (2024): "AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error" ‚Äî *Proc. IEEE/CVPR (Conference on Computer Vision and Pattern Recognition)*
* Sun K. (2024): "DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion" ‚Äî *Advances in Neural Information Processing Systems (NeurIPS / NeurIPS Proceedings)*

**Trabalhos Emp√≠ricos, Heur√≠sticos e Otimiza√ß√µes Algor√≠tmicas**

* Meena K.B. (2021): "A deep learning based method for image splicing detection" ‚Äî *Journal of Physics: Conference Series*
* Tripathi E. (2024): "An efficient digital image forgery detection using Pelican search optimisation-based DCNN" ‚Äî *Journal of Experimental and Theoretical Artificial Intelligence*
* Tripathi E.; (outros trabalhos forenses/heur√≠sticos relacionados) ‚Äî (veja se√ß√£o Forense & Otimiza√ß√£o)

**Estudos Forenses & Processamento de Imagem (2023‚Äì2021)**

* Kadha V. (2023): "Forensic analysis of manipulation chains: A deep residual network for detecting JPEG-manipulation-JPEG" ‚Äî *Forensic Science International: Digital Investigation*
* Chen J. (2023): "Identification of image global processing operator chain based on feature decoupling" ‚Äî *Information Sciences*
* Hassan A. (2021): "Texture based Image Splicing Forgery Recognition using a Passive Approach" ‚Äî *International Journal of Integrated Engineering*

**Confer√™ncias, Cole√ß√µes e Misc**

* ICSPIS (2022): *2022 5th International Conference on Signal Processing and Information Security (ICSPIS 2022)* ‚Äî (cole√ß√£o de trabalhos relevantes)
* Blond√© P. (2021): "In Medio Stat Virtus: intermediate levels of mind wandering improve episodic memory encoding in a virtual environment" ‚Äî *Psychological Research*
* Wang Z. (2019): "Image forgery detection algorithm based on U-shaped detection network" ‚Äî *Tongxin Xuebao Journal on Communications*

---


## ‚ùì **Quest√µes de Pesquisa (QP)**

### **üîç Quest√£o Principal:**

**"Como a an√°lise de complexidade-entropia pode aprimorar a detec√ß√£o de m√≠dias sint√©ticas em v√≠deos, superando as limita√ß√µes de generaliza√ß√£o dos m√©todos atuais baseados em deep learning?"**

### **üìã Quest√µes Secund√°rias (QS)**

**QS1:** Quais s√£o as assinaturas estat√≠sticas distintivas de v√≠deos sint√©ticos no espa√ßo complexidade-entropia comparadas √†s de v√≠deos aut√™nticos?

**QS2:** Como a fus√£o de features de complexidade-entropia com representa√ß√µes de Vision Transformers impacta na capacidade de generaliza√ß√£o cross-dataset?

**QS3:** Qual √© a robustez das features baseadas em entropia de permuta√ß√£o contra degrada√ß√µes comuns (compress√£o, ru√≠do) em v√≠deos?

**QS4:** Como os par√¢metros de embedding (dx, dy) influenciam na separabilidade entre classes no Plano CH?

**QS5:** Qual √© o trade-off entre interpretabilidade e performance dos detectores h√≠bridos propostos comparados aos m√©todos estado-da-arte?

**QS6:** Como as caracter√≠sticas temporais dos v√≠deos deepfake se manifestam atrav√©s da an√°lise de s√©ries temporais de complexidade-entropia?

### **üìä Crit√©rios de Avalia√ß√£o da Literatura (QA)**

#### **üî¨ Rigor Metodol√≥gico**

**Q1:** O estudo reporta m√©tricas de avalia√ß√£o claras e apropriadas para a tarefa (ex: Acur√°cia, AUC-ROC, EER)?
- **An√°lise:** Avalia o **Rigor** e a **Qualidade do Relato**. Garante que os estudos utilizam m√©tricas consolidadas, permitindo compara√ß√£o justa e quantitativa.

**Q2:** O estudo utiliza datasets p√∫blicos e bem conhecidos para valida√ß√£o (ex: FaceForensics++, Celeb-DF)?
- **An√°lise:** Avalia a **Credibilidade** e o **Rigor**. O uso de datasets p√∫blicos √© crucial para reprodutibilidade e valida√ß√£o em cen√°rios reconhecidos pela comunidade cient√≠fica.

**Q3:** O m√©todo proposto √© comparado com pelo menos um outro m√©todo de detec√ß√£o j√° existente (baseline)?
- **An√°lise:** Mede a **Relev√¢ncia** e o **Rigor** do estudo. Sem compara√ß√£o com baseline, √© imposs√≠vel aferir se a contribui√ß√£o √© de fato um avan√ßo.

#### **üéØ Robustez e Aplicabilidade**

**Q4:** O estudo avalia a robustez do detector contra perturba√ß√µes comuns (ex: compress√£o, ru√≠do, varia√ß√µes de ilumina√ß√£o)?
- **An√°lise:** Diretamente ligada ao "Outcome" do PICOC (aumento da robustez). Avalia a **Credibilidade** e **Relev√¢ncia** para aplica√ß√µes no mundo real.

**Q5:** A metodologia proposta √© descrita com detalhes suficientes para permitir a sua replica√ß√£o?
- **An√°lise:** Avalia a **Reprodutibilidade**. Se um artigo n√£o descreve claramente a metodologia, sua contribui√ß√£o cient√≠fica √© limitada.

#### **üìà Credibilidade Cient√≠fica**

**Q6:** Os autores discutem as limita√ß√µes do estudo e as amea√ßas √† validade dos resultados?
- **An√°lise:** O reconhecimento de limita√ß√µes demonstra **maturidade acad√™mica** e aumenta a credibilidade, indicando compreens√£o profunda do m√©todo.

**Q7:** Os objetivos da pesquisa, as contribui√ß√µes e as quest√µes de pesquisa do estudo est√£o claramente definidos?
- **An√°lise:** Garante que o artigo tem **foco claro** e contribui√ß√£o bem definida, evitando trabalhos com escopo vago ou objetivos pouco claros.

#### **üéØ Avalia√ß√£o Hol√≠stica**
Estas 7 quest√µes criam uma avalia√ß√£o completa que analisa:
- **"O qu√™"** (resultados e m√©tricas)
- **"Como"** (metodologia e relato)
- **"Por qu√™"** (relev√¢ncia e limita√ß√µes)

### **üìä Resultados Preliminares da Avalia√ß√£o QA**

üìã **Arquivo Completo de Resultados:** [RESULTADOS_QA.md](docs/picoc/qa/RESULTADOS_QA.md)  
üîç **√çndice de Navega√ß√£o:** [INDICE_ARTIGOS.md](docs/picoc/qa/INDICE_ARTIGOS.md)

> **üí° Navega√ß√£o:** Clique nos links da tabela abaixo para acessar diretamente os PDFs dos artigos ou suas avalia√ß√µes detalhadas. Cada artigo foi avaliado usando 7 crit√©rios rigorosos de qualidade acad√™mica.

#### **üèÜ Artigos Aprovados (10/10 - 100%)**

| # | Artigo | Pontua√ß√£o | PDF | Avalia√ß√£o |
|---|--------|-----------|-----|-----------|
| 1 | **Customized Transformer Adapter With Frequency Masking** | 6.5/8.0 | [üìÑ PDF](docs/picoc/aprovados/8/Customized_Transformer_Adapter_With_Frequency_Masking_for_Deepfake_Detection.pdf) | [üìä QA](docs/picoc/qa/Customized%20Transformer%20Adapter%20With%20Frequency%20Masking%20for%20Deepfake%20Detection) |
| 2 | **Joint spatial-frequency deepfake detection network** | 6.5/8.0 | [üìÑ PDF](docs/picoc/aprovados/2/s10489-025-06761-2.pdf) | [üìä QA](docs/picoc/qa/Joint%20spatial-frequency%20deepfake%20detection%20network%20based%20on%20dual-domain%20attention-enhanced%20deformable%20convolution) |
| 3 | **Detecting face tampering in videos using deepfake forensics** | 6.5/8.0 | [üìÑ PDF](docs/picoc/aprovados/11/Detecting%20face%20tampering%20in%20videos%20using%20deepfake%20forensics.pdf) | [üìä QA](docs/picoc/qa/Detecting%20face%20tampering%20in%20videos%20using%20deepfake%20forensics) |
| 4 | **Unravelling Digital Forgeries: Systematic Survey** | 6.5/8.0 | [üìÑ PDF](docs/picoc/aprovados/3/Unravelling%20Digital%20Forgeries:%20A%20Systematic%20Survey%20on%20Image%20Manipulation%20Detection%20and%20Localization.pdf) | [üìä QA](docs/picoc/qa/Unravelling%20Digital%20Forgeries%20A%20Systematic%20Survey%20on%20Image%20Manipulation%20Detection%20and%20Localization) |
| 5 | **DPL: Cross-quality DeepFake Detection** | 6.0/8.0 | [üìÑ PDF](docs/picoc/aprovados/1/s10791-025-09550-0.pdf) | [üìä QA](docs/picoc/qa/DPL%20Cross-quality%20DeepFake%20Detection%20via%20Dual%20Progressive%20Learning) |
| 6 | **SUMI-IFL: Information-Theoretic Framework** | 6.0/8.0 | [üìÑ PDF](docs/picoc/aprovados/5/32054-Article%20Text-36122-1-2-20250410.pdf) | [üìä QA](docs/picoc/qa/SUMI-IFL%20An%20Information-Theoretic%20Framework%20for%20Image%20Forgery%20Localization%20with%20Sufficiency%20and%20Minimality%20Constraints) |
| 7 | **LEAD-AI: lightweight entropy analysis** | 5.0/8.0 | [üìÑ PDF](docs/picoc/aprovados/9/LEAD-AI_%20lightweight%20entropy%20analysis%20for%20distinguishing%20AI-generated%20images%20from%20genuine%20photographs.pdf) | [üìä QA](docs/picoc/qa/LEAD-AI%20lightweight%20entropy%20analysis%20for%20distinguishing%20AI-generated%20images%20from%20genuine%20photographs) |
| 8 | **Exploring coordinated motion patterns** | 4.5/8.0 | [üìÑ PDF](docs/picoc/aprovados/6/Exploring%20coordinated%20motion%20patterns%20of%20facial%20landmarks%20for%20deepfake%20video%20detection%20-%20ScienceDirect.pdf) | [üìä QA](docs/picoc/qa/Exploring%20coordinated%20motion%20patterns%20of%20facial%20landmarks%20for%20deepfake%20video%20detection) |
| 9 | **Markov Observation Models and Deepfakes** | 4.5/8.0 | [üìÑ PDF](docs/picoc/aprovados/4/mathematics-13-02128-v2.pdf) | [üìä QA](docs/picoc/qa/Markov%20Observation%20Models%20and%20Deepfakes) |
| 10 | **A survey on multimedia-enabled deepfake detection** | 4.5/8.0 | üìö Survey | [üìä QA](docs/picoc/qa/A%20survey%20on%20multimedia-enabled%20deepfake%20detection%20state-of-the-art%20tools%20and%20techniques,%20emerging%20trends,%20current%20challenges%20&%20limitations,%20and%20future%20directions) |
| 11 | **Continual Face Forgery Detection via Historical Distribution Preserving** | 5.0/8.0 | [üìÑ PDF](docs/picoc/aprovados/7/Continual%20Face%20Forgery%20Detection%20via%20Historical%20Distribution%20Preserving.pdf) | [üìä QA](docs/picoc/qa/Continual%20Face%20Forgery%20Detection%20via%20Historical%20Distribution%20Preserving) |

#### **üìà Estat√≠sticas Gerais**
- **Taxa de Aprova√ß√£o:** 100% (10/10 artigos)
- **Pontua√ß√£o M√©dia:** 5.8/8.0
- **Melhor Desempenho:** Q1 e Q7 (100% de conformidade)
- **√Årea de Melhoria:** Q6 - Discuss√£o de limita√ß√µes (40% de conformidade)

---


## üîß **Ambiente de Desenvolvimento**

### **üêç Ambiente Anaconda**
O projeto utiliza **Python** como linguagem principal, gerenciado atrav√©s do **Anaconda** para garantir reprodutibilidade e isolamento de depend√™ncias.

#### **Instala√ß√£o do Ambiente:**
```bash
# Criar ambiente conda para detec√ß√£o de m√≠dias sint√©ticas
conda create -n ia-product python=3.9 -y

# Ativar o ambiente
conda activate ia-product

# Instalar depend√™ncias principais
conda install numpy pandas matplotlib scikit-learn jupyter -y

# Instalar PyTorch (vers√£o compat√≠vel com CUDA se dispon√≠vel)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# Instalar bibliotecas de vis√£o computacional e ML
pip install opencv-python pillow transformers timm

# Instalar ordpy para an√°lise de entropia de permuta√ß√£o
pip install ordpy

# Instalar bibliotecas adicionais para o projeto
pip install xgboost lightgbm seaborn plotly

# Verificar instala√ß√£o
python -c "import torch, ordpy, cv2; print('Ambiente configurado com sucesso!')"
```


## üìä **Cronograma**

O projeto est√° planejado para execu√ß√£o ao longo de **24 meses**, dividido em quatro fases:

### **üìö Fase 1 (Meses 1-6): Fundamenta√ß√£o e Implementa√ß√£o**
- Revis√£o aprofundada da literatura
- Configura√ß√£o do ambiente computacional (Anaconda + ordpy)
- Implementa√ß√£o dos pipelines F_CH e F_ViT
- Familiariza√ß√£o com datasets

### **üî¨ Fase 2 (Meses 7-12): Experimenta√ß√£o**
- Extra√ß√£o de features nos datasets FF++ e Celeb-DF
- An√°lise de sensibilidade dos par√¢metros PE2D
- Caracteriza√ß√£o das assinaturas de complexidade
- Valida√ß√£o da Hip√≥tese de Separa√ß√£o (H1)

### **ü§ñ Fase 3 (Meses 13-18): Desenvolvimento**
- Desenvolvimento do modelo h√≠brido
- Implementa√ß√£o do modelo baseline
- Treinamento e otimiza√ß√£o
- Valida√ß√£o das hip√≥teses H2 e H3

### **üìä Fase 4 (Meses 19-24): Valida√ß√£o e Documenta√ß√£o**
- Protocolo de valida√ß√£o final
- Testes de generaliza√ß√£o e robustez
- An√°lise dos resultados
- Reda√ß√£o da disserta√ß√£o

---

## üìà **Resultados Esperados**  

- **Valida√ß√£o Emp√≠rica:** Confirma√ß√£o das tr√™s hip√≥teses centrais do projeto
- **Framework Inovador:** Desenvolvimento de um detector h√≠brido fundamentado em teoria
- **Generaliza√ß√£o Superior:** Desempenho robusto em datasets n√£o vistos durante treinamento
- **Interpretabilidade:** Explica√ß√µes claras dos mecanismos de detec√ß√£o
- **Contribui√ß√£o Cient√≠fica:** Publica√ß√µes em confer√™ncias e peri√≥dicos de alto impacto
- **C√≥digo Aberto:** Disponibiliza√ß√£o do framework para a comunidade cient√≠fica

---

## üìö **Refer√™ncias Bibliogr√°ficas**

AGARWAL, S. et al. Detecting face synthesis using convolutional neural networks and image quality assessment. **IEEE Transactions on Information Forensics and Security**, v. 15, p. 3044-3055, 2020.

AFCHAR, D. et al. MesoNet: a Compact Facial Video Forgery Detection Network. In: **IEEE International Workshop on Information Forensics and Security (WIFS)**. Hong Kong: IEEE, 2018. p. 1-7. DOI: [10.1109/WIFS.2018.8630761](https://doi.org/10.1109/WIFS.2018.8630761).

AMERINI, I. et al. Deepfake-o-meter: An open platform for deepfake detection. In: **Proceedings of the 29th ACM International Conference on Multimedia**. Virtual Event: ACM, 2021. p. 103-112. DOI: [10.1145/3474085.3475667](https://doi.org/10.1145/3474085.3475667).

ANDERSON, R. J. **Security Engineering: A Guide to Building Dependable Distributed Systems**. 3. ed. Hoboken: John Wiley & Sons, 2020.

ANTUNES, P. et al. Leveraging ordinal patterns for improved deepfake detection. **Neural Computing and Applications**, v. 34, n. 18, p. 15479-15493, 2022. DOI: [10.1007/s00521-022-07043-5](https://doi.org/10.1007/s00521-022-07043-5).

BANDT, C.; POMPE, B. Permutation entropy: a natural complexity measure for time series. **Physical Review Letters**, v. 88, n. 17, p. 174102, 2002. DOI: [10.1103/PhysRevLett.88.174102](https://doi.org/10.1103/PhysRevLett.88.174102).

BONETTINI, N. et al. Video face manipulation detection through ensemble of CNNs. In: **International Conference on Pattern Recognition (ICPR)**. Milan: IEEE, 2020. p. 5012-5019. DOI: [10.1109/ICPR48806.2021.9412711](https://doi.org/10.1109/ICPR48806.2021.9412711).

BROWN, T. et al. Language models are few-shot learners. In: **Advances in Neural Information Processing Systems**, v. 33, p. 1877-1901, 2020. Dispon√≠vel em: [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf). Acesso em: 26 ago. 2025.

CALDELLI, R.; BECARELLI, R.; AMERINI, I. Image origin classification based on social network provenance. **IEEE Transactions on Information Forensics and Security**, v. 12, n. 6, p. 1299-1308, 2017. DOI: [10.1109/TIFS.2017.2656842](https://doi.org/10.1109/TIFS.2017.2656842).

CHEN, S. et al. The eyes tell all: detecting fake face images via the eyes. **IEEE Access**, v. 8, p. 149915-149924, 2020. DOI: [10.1109/ACCESS.2020.3016867](https://doi.org/10.1109/ACCESS.2020.3016867).

CHOLLET, F. Xception: Deep learning with depthwise separable convolutions. In: **Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition**. Honolulu: IEEE, 2017. p. 1251-1258. DOI: [10.1109/CVPR.2017.195](https://doi.org/10.1109/CVPR.2017.195).

DOLHANSKY, B. et al. The DeepFake Detection Challenge (DFDC) Dataset and Benchmark. **arXiv preprint** arXiv:2006.07397, 2020. Dispon√≠vel em: [https://arxiv.org/abs/2006.07397](https://arxiv.org/abs/2006.07397). Acesso em: 26 ago. 2025.

DOSOVITSKIY, A. et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In: **International Conference on Learning Representations (ICLR)**. Vienna: OpenReview, 2021. Dispon√≠vel em: [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy). Acesso em: 26 ago. 2025.

DURALL, R. et al. Watch your up-convolution: CNN based generative deep neural networks are failing to reproduce spectral distributions. In: **Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition**. Seattle: IEEE, 2020. p. 7890-7899. DOI: [10.1109/CVPR42600.2020.00791](https://doi.org/10.1109/CVPR42600.2020.00791).

FRANK, J.; EISENHOFER, T.; SCH√ñNHERR, L. Leveraging frequency analysis for deep fake image recognition. In: **International Conference on Machine Learning**. PMLR, 2020. p. 3247-3258. Dispon√≠vel em: [http://proceedings.mlr.press/v119/frank20a.html](http://proceedings.mlr.press/v119/frank20a.html). Acesso em: 26 ago. 2025.

GOODFELLOW, I. et al. Generative Adversarial Nets. In: **Advances in Neural Information Processing Systems**, v. 27, p. 2672-2680, 2014. Dispon√≠vel em: [https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf). Acesso em: 26 ago. 2025.

GUARNERA, L. et al. Deepfake video detection through optical flow based CNN. In: **Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops**. Seoul: IEEE, 2019. p. 1205-1207. DOI: [10.1109/ICCVW.2019.00152](https://doi.org/10.1109/ICCVW.2019.00152).

HE, K. et al. Deep residual learning for image recognition. In: **Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition**. Las Vegas: IEEE, 2016. p. 770-778. DOI: [10.1109/CVPR.2016.90](https://doi.org/10.1109/CVPR.2016.90).

HEUSEL, M. et al. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In: **Advances in Neural Information Processing Systems**, v. 30, 2017. Dispon√≠vel em: [https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf). Acesso em: 26 ago. 2025.

JIANG, L. et al. Celeb-DF: A large-scale challenging dataset for deepfake forensics. In: **Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition**. Seattle: IEEE, 2020. p. 3207-3216. DOI: [10.1109/CVPR42600.2020.00327](https://doi.org/10.1109/CVPR42600.2020.00327).

KARRAS, T. et al. Analyzing and improving the image quality of StyleGAN. In: **Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition**. Seattle: IEEE, 2020. p. 8110-8119. DOI: [10.1109/CVPR42600.2020.00813](https://doi.org/10.1109/CVPR42600.2020.00813).

LI, L. et al. Face X-ray for more general face forgery detection. In: **Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition**. Seattle: IEEE, 2020. p. 5001-5010. DOI: [10.1109/CVPR42600.2020.00505](https://doi.org/10.1109/CVPR42600.2020.00505).

LI, Y. et al. In ictu oculi: Exposing AI generated fake face videos by detecting eye blinking. In: **IEEE International Workshop on Information Forensics and Security (WIFS)**. Hong Kong: IEEE, 2018. p. 1-7. DOI: [10.1109/WIFS.2018.8630787](https://doi.org/10.1109/WIFS.2018.8630787).

LOPEZ-PAZ, D.; OQUAB, M. Revisiting classifier two-sample tests. In: **International Conference on Learning Representations**. Toulon: OpenReview, 2017. Dispon√≠vel em: [https://openreview.net/forum?id=SJkXfE5xx](https://openreview.net/forum?id=SJkXfE5xx). Acesso em: 26 ago. 2025.

## üì¨ **Contato**

üì© **E-mail:** [fl@ic.ufal.br](mailto:fl@ic.ufal.br)  
üîó **LinkedIn:** [linkedin.com/in/fabio-linhares](https://www.linkedin.com/in/fabio-linhares)  
üêô **GitHub:** [github.com/fabio-linhares](https://github.com/fabio-linhares)  
üåê **Site do Projeto:** [fabiolinhares.com.br/ufal/orientacao/preprojeto](https://www.fabiolinhares.com.br/ufal/orientacao/preprojeto/preprojeto.html)

---

**Trabalho de Mestrado - Programa de P√≥s-Gradua√ß√£o em Inform√°tica**  
**Universidade Federal de Alagoas (UFAL)**  
**Orientador:** Prof.¬™ Dr.¬™ Fabiane da Silva Queiroz  
**Ano:** 2025
