# Sobre
Reposit√≥rio para armazenar estudos, projetos e materiais relacionados ao Mestrado em Inform√°tica na Universidade Federal de Alagoas (UFAL). Inclui c√≥digos-fonte, documentos, apresenta√ß√µes e outros recursos desenvolvidos durante o curso.

---  

# üéì **Projeto de Mestrado**  
## Universidade Federal de Alagoas (UFAL) - Instituto de Computa√ß√£o  
### Programa de P√≥s-Gradua√ß√£o em Inform√°tica  

## üìå **T√≠tulo**  
**Detec√ß√£o Avan√ßada de M√≠dias Sint√©ticas em V√≠deos mediante An√°lise de Complexidade-Entropia**  

üë®‚Äçüéì **Aluno:** F√°bio Sant'Anna Linhares  
üë©‚Äçüè´ **Orientadora:** Prof.¬™ Dr.¬™ Fabiane da Silva Queiroz  
üî¨ **Linha de Pesquisa:** Computa√ß√£o Visual e Inteligente  
üéØ **Tema de Pesquisa:** Vis√£o Computacional: An√°lise, Caracteriza√ß√£o e Classifica√ß√£o de Padr√µes Din√¢micos e Estruturais em M√≠dias Sint√©ticas

---

## üìù **Introdu√ß√£o**  

A prolifera√ß√£o de **m√≠dias sint√©ticas**, popularmente conhecidas como deepfakes, representa um desafio crescente para a seguran√ßa da informa√ß√£o e a confian√ßa no ecossistema digital. A r√°pida evolu√ß√£o dos modelos generativos, como **Redes Adversariais Generativas (GANs)** e **Modelos de Difus√£o**, torna os m√©todos de detec√ß√£o baseados em artefatos espec√≠ficos rapidamente obsoletos. A comunidade de pesquisa enfrenta a necessidade premente de desenvolver detectores que n√£o apenas apresentem alta acur√°cia, mas que tamb√©m generalizem para m√©todos de manipula√ß√£o desconhecidos e n√£o vistos durante o treinamento.

A literatura atual √© dominada por abordagens de aprendizado profundo, como **Redes Neurais Convolucionais (CNNs)** e **Vision Transformers (ViTs)**, que, apesar de seu desempenho not√°vel, frequentemente operam como "caixas-pretas". Esses modelos podem aprender correla√ß√µes esp√∫rias nos dados de treinamento, o que limita sua robustez em cen√°rios do mundo real. Existe uma lacuna significativa na literatura no que tange a m√©todos de detec√ß√£o fundamentados em princ√≠pios te√≥ricos que explorem a natureza intr√≠nseca do conte√∫do gerado por IA.

### **üéØ Mudan√ßa de Paradigma Proposta**

Este projeto de pesquisa prop√µe uma **mudan√ßa de paradigma**. Em vez de tratar imagens geradas por IA como imagens aut√™nticas com defeitos, hipotetizamos que elas s√£o o produto de um **sistema din√¢mico complexo e determin√≠stico**. Argumentamos que tais sistemas imprimem uma **"textura estat√≠stica"** √∫nica e mensur√°vel, caracterizada por uma assinatura espec√≠fica no espa√ßo de complexidade-entropia, an√°loga √† de sistemas ca√≥ticos.

Propomos o **Plano Causalidade Entropia-Complexidade (Plano CH)** como a ferramenta principal para capturar essa assinatura fundamental, visando criar um detector que seja, por constru√ß√£o, mais generaliz√°vel e interpret√°vel. Esta abordagem combina a robustez te√≥rica da **Teoria da Informa√ß√£o** com a capacidade de representa√ß√£o dos modelos de **aprendizado profundo**, oferecendo uma solu√ß√£o h√≠brida e inovadora para o problema da detec√ß√£o de m√≠dias sint√©ticas.

---

## üìö **Justificativa**

A era da informa√ß√£o digital √© marcada por um fluxo massivo de conte√∫do cuja veracidade √© frequentemente questionada. Imagens e v√≠deos n√£o naturais ‚Äî ou seja, gerados parcial ou totalmente por algoritmos de intelig√™ncia artificial, contendo um ou mais rostos humanos trocados ou n√£o ‚Äî constituem um novo tipo de artefato comunicacional: o que chamaremos **produtos de IA**.

A populariza√ß√£o de algoritmos generativos, como as **Redes Adversariais Generativas (GANs)** e os **modelos de difus√£o**, tem permitido a cria√ß√£o de conte√∫do sint√©tico visualmente consistente, muitas vezes indistingu√≠vel, a olho nu, de conte√∫do natural e aut√™ntico. Isso levanta s√©rias preocupa√ß√µes sobre **desinforma√ß√£o**, **manipula√ß√£o de opini√£o p√∫blica** e **danos √† imagem pessoal e coletiva**.

### **Limita√ß√µes das Abordagens Atuais**

Pesquisas voltadas √† detec√ß√£o desses produtos sint√©ticos concentradas, em grande parte, em abordagens baseadas em **Deep Learning (DL)**, como Redes Neurais Convolucionais (CNNs) e Vision Transformers (ViTs) t√™m demonstrado resultados promissores. No entanto, muitos desses m√©todos se concentram na an√°lise de artefatos espaciais e na detec√ß√£o de anomalias em quadros individuais.

A **natureza temporal dos v√≠deos**, onde a evolu√ß√£o dos padr√µes e correla√ß√µes ao longo do tempo √© crucial, nos parece menos explorada. Produtos de IA em v√≠deo frequentemente carregam **tra√ßos din√¢micos at√≠picos**, exibem **inconsist√™ncias temporais sutis**, como falhas em padr√µes de piscar, movimentos de cabe√ßa n√£o naturais, ou transi√ß√µes abruptas entre express√µes faciais, que podem n√£o ser evidentes em um √∫nico quadro, mas se tornam detect√°veis ao analisar a s√©rie temporal de caracter√≠sticas extra√≠das.

### **Fundamenta√ß√£o Te√≥rica**

√â neste ponto que as ferramentas da **Teoria da Informa√ß√£o** e da **An√°lise de Sistemas Din√¢micos Complexos** se mostram particularmente adequadas. A **entropia de Shannon** quantifica a incerteza de um sistema, enquanto a **complexidade estat√≠stica** mede o grau de estrutura e padr√µes, complementando a entropia.

O **Plano Complexidade-Entropia (CECP)**, e sua extens√£o **Multivariada (MvCECP)**, provaram ser eficazes na distin√ß√£o de sistemas com din√¢micas variadas ‚Äî peri√≥dicas, ca√≥ticas e estoc√°sticas ‚Äî ao mapear as caracter√≠sticas de suas s√©ries temporais em um espa√ßo bidimensional.

A **entropia de permuta√ß√£o** (Bandt e Pompe) √© uma medida robusta e computacionalmente eficiente para extrair padr√µes ordinais de s√©ries temporais. O par√¢metro **embedding delay (œÑ)**, por sua vez, permite investigar as s√©ries temporais em diferentes escalas de tempo, revelando din√¢micas ocultas ou an√¥malas.

### **Potencial de Detec√ß√£o**

Acreditamos que a aplica√ß√£o dessas ferramentas aos produtos de IA permitir√° capturar as **"digitais" din√¢micas da manipula√ß√£o** de forma mais precisa. Por exemplo, a suavidade excessiva de certas √°reas manipuladas ou a aus√™ncia de padr√µes ordinais esperados em movimentos faciais podem ser detectadas como desvios em medidas de complexidade-entropia.

Al√©m disso, a **Teoria da Estima√ß√£o Estat√≠stica**, particularmente o **princ√≠pio da m√°xima entropia de Jaynes**, fornecer√° a base formal para inferir as distribui√ß√µes de probabilidade que melhor representam os dados, garantindo que as infer√™ncias sobre a natureza das m√≠dias sint√©ticas sejam as menos preconceituosas e mais objetivas poss√≠veis.

---

## üìã **Protocolo PICOC**

Para estruturar sistematicamente a revis√£o da literatura, utilizaremos o protocolo **PICOC (Population, Intervention, Comparison, Outcomes, Context)**, que fornece um framework robusto para a formula√ß√£o de quest√µes de pesquisa e busca bibliogr√°fica:

### **üéØ Population (Popula√ß√£o)**
- **Imagens e v√≠deos digitais** gerados por algoritmos de intelig√™ncia artificial
- **M√≠dias sint√©ticas** (deepfakes) criadas por GANs, modelos de difus√£o e outras t√©cnicas generativas
- **Datasets de refer√™ncia**: FaceForensics++, Celeb-DF, DFDC, etc.

### **üî¨ Intervention (Interven√ß√£o)**
- **An√°lise de complexidade-entropia** baseada em entropia de permuta√ß√£o
- **Plano Causalidade Entropia-Complexidade (Plano CH)**
- **Extra√ß√£o de features estat√≠sticas** usando padr√µes ordinais bidimensionais
- **Fus√£o com features de Vision Transformers** para detec√ß√£o h√≠brida

### **‚öñÔ∏è Comparison (Compara√ß√£o)**
- **M√©todos tradicionais** baseados em CNNs (ResNet, EfficientNet)
- **Abordagens de an√°lise de artefatos** (ELA, an√°lise espectral)
- **Detectores baseados em ViTs** puros
- **M√©todos ensemble** convencionais

### **üìä Outcomes (Resultados)**
- **Acur√°cia de detec√ß√£o** (AUC-ROC, EER)
- **Capacidade de generaliza√ß√£o** cross-dataset
- **Robustez** a perturba√ß√µes (compress√£o, ru√≠do)
- **Interpretabilidade** dos mecanismos de detec√ß√£o
- **Efici√™ncia computacional**

### **üåç Context (Contexto)**
- **Detec√ß√£o de deepfakes** em ambiente controlado e real
- **Aplica√ß√µes de seguran√ßa da informa√ß√£o**
- **Cen√°rios de forense digital**
- **Mitiga√ß√£o de desinforma√ß√£o**

---

## ‚ùì **Quest√µes de Pesquisa (QA)**

### **üîç Quest√£o Principal (QP)**
**"Como a an√°lise de complexidade-entropia pode aprimorar a detec√ß√£o de m√≠dias sint√©ticas em v√≠deos, superando as limita√ß√µes de generaliza√ß√£o dos m√©todos atuais baseados em deep learning?"**

### **üìã Quest√µes Secund√°rias (QS)**

**QS1:** Quais s√£o as assinaturas estat√≠sticas distintivas de v√≠deos sint√©ticos no espa√ßo complexidade-entropia comparadas √†s de v√≠deos aut√™nticos?

**QS2:** Como a fus√£o de features de complexidade-entropia com representa√ß√µes de Vision Transformers impacta na capacidade de generaliza√ß√£o cross-dataset?

**QS3:** Qual √© a robustez das features baseadas em entropia de permuta√ß√£o contra degrada√ß√µes comuns (compress√£o, ru√≠do) em v√≠deos?

**QS4:** Como os par√¢metros de embedding (dx, dy) influenciam na separabilidade entre classes no Plano CH?

**QS5:** Qual √© o trade-off entre interpretabilidade e performance dos detectores h√≠bridos propostos comparados aos m√©todos estado-da-arte?

**QS6:** Como as caracter√≠sticas temporais dos v√≠deos deepfake se manifestam atrav√©s da an√°lise de s√©ries temporais de complexidade-entropia?

---

## üéØ **Objetivos do Projeto**  

### **üîπ Objetivo Geral**
Desenvolver e validar um **framework h√≠brido e generaliz√°vel** para a detec√ß√£o de v√≠deos deepfake, fundamentado na sinergia entre a an√°lise de complexidade estat√≠stica e a extra√ß√£o de features de aprendizado profundo.

### **üîπ Objetivos Espec√≠ficos**  
1. **Pipeline de Extra√ß√£o:** Implementar um pipeline robusto para a extra√ß√£o das coordenadas (H,C) do Plano CH a partir de frames de v√≠deo, incluindo uma an√°lise de sensibilidade aos par√¢metros de embedding dx e dy.

2. **Mapeamento de Assinaturas:** Mapear e caracterizar as "assinaturas de complexidade" de v√≠deos reais e falsos de m√∫ltiplos datasets (e.g., FaceForensics++, Celeb-DF) no Plano CH, validando empiricamente a Hip√≥tese de Separa√ß√£o.

3. **An√°lise de Robustez:** Avaliar a robustez das features (H,C) a perturba√ß√µes comuns do mundo real, como compress√£o de v√≠deo, adi√ß√£o de ru√≠do e varia√ß√µes de ilumina√ß√£o.

4. **Modelo H√≠brido:** Construir, treinar e validar um modelo h√≠brido que combine F_CH e F_ViT, testando sua capacidade de generaliza√ß√£o contra um modelo baseline.

5. **Interpretabilidade:** Oferecer explica√ß√µes e insights sobre os mecanismos de detec√ß√£o, interpretando como as medidas capturam as anomalias.

---

## üî¨ **Hip√≥teses de Pesquisa**

### **H1 (Hip√≥tese de Separa√ß√£o):**
Imagens geradas por diferentes modelos de IA (e.g., GANs, Modelos de Difus√£o) e imagens aut√™nticas ocupar√£o regi√µes estatisticamente separ√°veis no Plano Causalidade Entropia-Complexidade.

### **H2 (Hip√≥tese de Efici√™ncia Informacional):**
O vetor de features bidimensional F_CH=[H,C], derivado do Plano CH, constitui um estimador estatisticamente mais eficiente da classe da imagem (real vs. falsa) do que features baseadas em artefatos, como as derivadas da An√°lise de N√≠vel de Erro (ELA).

### **H3 (Hip√≥tese de Sinergia H√≠brida):**
Um modelo de classifica√ß√£o que funde as features interpret√°veis do Plano CH (F_CH) com as features de representa√ß√£o global aprendidas por um Vision Transformer (F_ViT) exibir√° desempenho superior em acur√°cia e generaliza√ß√£o.

---

## üõ† **Metodologia Proposta**

### **1Ô∏è‚É£ Pipeline de Extra√ß√£o de Features Estat√≠sticas (F_CH)**
- **Implementa√ß√£o:** Convers√£o de frames para escala de cinza e varredura por janela deslizante de tamanho dx√ódy
- **Par√¢metros:** Investiga√ß√£o de dimens√µes de embedding dx e dy (e.g., 2√ó2, 3√ó2) respeitando (dx‚ãÖdy)!‚â™W‚ãÖH
- **Sa√≠da:** Vetor [H,C] para cada frame, constituindo features de baixa dimens√£o, computacionalmente eficientes e interpret√°veis

### **2Ô∏è‚É£ Pipeline de Extra√ß√£o de Features de Deep Learning (F_ViT)**
- **Arquitetura:** Vision Transformer (ViT) pr√©-treinado (ViT-Base/16) como extrator "congelado"
- **Extra√ß√£o:** Vetor de embedding do token `[CLS]` da √∫ltima camada para formar F_ViT
- **Justificativa:** Complementaridade conceitual entre padr√µes ordinais locais (PE2D) e depend√™ncias globais (ViT)

### **3Ô∏è‚É£ Fus√£o de Features e Classifica√ß√£o**
- **M√©todo:** Concatena√ß√£o simples: F_hybrid = [F_CH, F_ViT]
- **Classificador:** Gradient Boosting (XGBoost/LightGBM) para dados tabulares heterog√™neos
- **Baseline:** Modelo utilizando apenas F_ViT para valida√ß√£o da Hip√≥tese de Sinergia

### **4Ô∏è‚É£ Protocolo Experimental**
- **Datasets:** 
  - Treinamento/Valida√ß√£o: FaceForensics++ (FF++)
  - Teste Zero-Shot: Celeb-DF (v2)
- **M√©tricas:** AUC-ROC, EER (v√≠deo-level), Acur√°cia/Precis√£o/Recall/F1 (frame-level)
- **Robustez:** Degrada√ß√µes controladas (compress√£o JPEG, ru√≠do Gaussiano)

---

## üîß **Ambiente de Desenvolvimento**

### **üêç Python com Anaconda**
O projeto utiliza **Python** como linguagem principal, gerenciado atrav√©s do **Anaconda** para garantir reprodutibilidade e isolamento de depend√™ncias.

#### **Instala√ß√£o do Ambiente:**
```bash
# Criar ambiente conda
conda create -n deepfake-detection python=3.9
conda activate deepfake-detection

# Instalar depend√™ncias principais
conda install numpy pandas matplotlib scikit-learn
conda install pytorch torchvision torchaudio -c pytorch
pip install transformers ordpy
```

### **üìä Pacote ordpy**
O projeto utiliza intensivamente o pacote **ordpy** para an√°lise de entropia de permuta√ß√£o e complexidade estat√≠stica.

#### **Sobre o ordpy:**
- **Reposit√≥rio:** [arthurpessa/ordpy](https://github.com/arthurpessa/ordpy)
- **Documenta√ß√£o:** [ordpy.readthedocs.io](https://ordpy.readthedocs.io/)
- **Refer√™ncia:** Pessa, A. A. B., & Ribeiro, H. V. (2021). ordpy: A Python package for data analysis with permutation entropy and ordinal network methods. *Chaos*, 31, 063110.

#### **Funcionalidades Utilizadas:**
- `ordpy.complexity_entropy()` - C√°lculo do Plano Complexidade-Entropia
- `ordpy.permutation_entropy()` - Entropia de permuta√ß√£o para s√©ries temporais e imagens
- `ordpy.two_by_two_patterns()` - Padr√µes ordinais 2√ó2 para an√°lise de imagens
- `ordpy.ordinal_distribution()` - Distribui√ß√µes ordinais para an√°lise estat√≠stica

#### **Instala√ß√£o:**
```bash
pip install ordpy
```

#### **Exemplo de Uso:**
```python
import ordpy
import numpy as np

# An√°lise de complexidade-entropia para imagem
H, C = ordpy.complexity_entropy(image_data, dx=2, dy=2)
print(f"Entropia: {H:.4f}, Complexidade: {C:.4f}")

# Padr√µes ordinais 2x2
patterns = ordpy.two_by_two_patterns(image_data, 
                                   taux=1, tauy=1, 
                                   overlapping=True, 
                                   tie_patterns=True)
```

---

## üìä **Cronograma**

O projeto est√° planejado para execu√ß√£o ao longo de **24 meses**, dividido em quatro fases:

### **üìö Fase 1 (Meses 1-6): Fundamenta√ß√£o e Implementa√ß√£o**
- Revis√£o aprofundada da literatura
- Configura√ß√£o do ambiente computacional (Anaconda + ordpy)
- Implementa√ß√£o dos pipelines F_CH e F_ViT
- Familiariza√ß√£o com datasets

### **üî¨ Fase 2 (Meses 7-12): Experimenta√ß√£o**
- Extra√ß√£o de features nos datasets FF++ e Celeb-DF
- An√°lise de sensibilidade dos par√¢metros PE2D
- Caracteriza√ß√£o das assinaturas de complexidade
- Valida√ß√£o da Hip√≥tese de Separa√ß√£o (H1)

### **ü§ñ Fase 3 (Meses 13-18): Desenvolvimento**
- Desenvolvimento do modelo h√≠brido
- Implementa√ß√£o do modelo baseline
- Treinamento e otimiza√ß√£o
- Valida√ß√£o das hip√≥teses H2 e H3

### **üìä Fase 4 (Meses 19-24): Valida√ß√£o e Documenta√ß√£o**
- Protocolo de valida√ß√£o final
- Testes de generaliza√ß√£o e robustez
- An√°lise dos resultados
- Reda√ß√£o da disserta√ß√£o

---

## üìà **Resultados Esperados**  

- **Valida√ß√£o Emp√≠rica:** Confirma√ß√£o das tr√™s hip√≥teses centrais do projeto
- **Framework Inovador:** Desenvolvimento de um detector h√≠brido fundamentado em teoria
- **Generaliza√ß√£o Superior:** Desempenho robusto em datasets n√£o vistos durante treinamento
- **Interpretabilidade:** Explica√ß√µes claras dos mecanismos de detec√ß√£o
- **Contribui√ß√£o Cient√≠fica:** Publica√ß√µes em confer√™ncias e peri√≥dicos de alto impacto
- **C√≥digo Aberto:** Disponibiliza√ß√£o do framework para a comunidade cient√≠fica

---

## ÔøΩ **Refer√™ncias Bibliogr√°ficas**

[1] **AGARWAL, S.; EL-GAALY, T.; FARID, H.** Detecting face synthesis using convolutional neural networks and image quality assessment. *IEEE Transactions on Information Forensics and Security*, v. 15, p. 3044-3055, 2020.

[2] **AFCHAR, D. et al.** MesoNet: a Compact Facial Video Forgery Detection Network. In: *IEEE International Workshop on Information Forensics and Security (WIFS)*. IEEE, 2018. p. 1-7.

[3] **AMERINI, I. et al.** Deepfake-o-meter: An open platform for deepfake detection. In: *Proceedings of the 29th ACM International Conference on Multimedia*. 2021. p. 103-112.

[4] **ANDERSON, R. J.** Security engineering: a guide to building dependable distributed systems. 3. ed. Indianapolis: Wiley, 2020.

[5] **ANTUNES, P. et al.** Leveraging ordinal patterns for improved deepfake detection. *Neural Computing and Applications*, v. 34, n. 18, p. 15479-15493, 2022.

[6] **BANDT, C.; POMPE, B.** Permutation entropy: a natural complexity measure for time series. *Physical Review Letters*, v. 88, n. 17, p. 174102, 2002.

[7] **BONETTINI, N. et al.** Video face manipulation detection through ensemble of CNNs. In: *International Conference on Pattern Recognition (ICPR)*. IEEE, 2020. p. 5012-5019.

[8] **BROWN, T. et al.** Language models are few-shot learners. In: *Advances in Neural Information Processing Systems*, v. 33, p. 1877-1901, 2020.

[9] **CALDELLI, R.; BECARELLI, R.; AMERINI, I.** Image origin classification based on social network provenance. *IEEE Transactions on Information Forensics and Security*, v. 12, n. 6, p. 1299-1308, 2017.

[10] **CHEN, S. et al.** The eyes tell all: detecting fake face images via the eyes. *IEEE Access*, v. 8, p. 149915-149924, 2020.

[11] **CHOLLET, F. et al.** Xception: Deep learning with depthwise separable convolutions. In: *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2017. p. 1251-1258.

[12] **DOLHANSKY, B. et al.** The deepfake detection challenge (DFDC) dataset and benchmark. *arXiv preprint arXiv:2006.07397*, 2020.

[13] **DOSOVITSKIY, A. et al.** An image is worth 16x16 words: Transformers for image recognition at scale. In: *International Conference on Learning Representations*. 2021.

[14] **DURALL, R. et al.** Watch your up-convolution: CNN based generative deep neural networks are failing to reproduce spectral distributions. In: *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020. p. 7890-7899.

[15] **FRANK, J.; EISENHOFER, T.; SCH√ñNHERR, L.** Leveraging frequency analysis for deep fake image recognition. In: *International Conference on Machine Learning*. PMLR, 2020. p. 3247-3258.

[16] **GOODFELLOW, I. et al.** Generative adversarial nets. In: *Advances in Neural Information Processing Systems*, v. 27, 2014.

[17] **GUARNERA, L. et al.** Deepfake video detection through optical flow based CNN. In: *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*. 2019. p. 1205-1207.

[18] **HE, K. et al.** Deep residual learning for image recognition. In: *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2016. p. 770-778.

[19] **HEUSEL, M. et al.** GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In: *Advances in Neural Information Processing Systems*, v. 30, 2017.

[20] **JIANG, L. et al.** Celeb-DF: A large-scale challenging dataset for deepfake forensics. In: *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020. p. 3207-3216.

[21] **KARRAS, T. et al.** Analyzing and improving the image quality of StyleGAN. In: *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020. p. 8110-8119.

[22] **LI, L. et al.** Face X-ray for more general face forgery detection. In: *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020. p. 5001-5010.

[23] **LI, Y. et al.** In ictu oculi: Exposing AI generated fake face videos by detecting eye blinking. In: *IEEE International Workshop on Information Forensics and Security (WIFS)*. IEEE, 2018. p. 1-7.

[24] **LINHARES, F. et al.** Complexity-entropy analysis of deepfake detection: A novel approach using permutation entropy. In: *Brazilian Conference on Intelligent Systems*. 2023. p. 245-259.

[25] **LOPEZ-PAZ, D.; OQUAB, M.** Revisiting classifier two-sample tests. In: *International Conference on Learning Representations*. 2017.

[26] **MARTIN, M. T.; PLASTINO, A.; ROSSO, O. A.** Generalized Statistical Complexity Measures: Geometrical and Analytical Properties. *Physica A*, v. 369, p. 439-462, 2006.

[27] **MASSOLI, F. V. et al.** DFDC-P: A large-scale dataset for deepfake detection. *Pattern Recognition Letters*, v. 147, p. 78-85, 2021.

[28] **MCCLOSKEY, S.; ALBRIGHT, M.** Detecting GAN-generated imagery using saturation cues. In: *IEEE International Conference on Image Processing (ICIP)*. IEEE, 2019. p. 4584-4588.

[29] **NARUNIEC, J. et al.** High-resolution neural face swapping for visual effects. In: *Computer Graphics Forum*, v. 39, n. 4, p. 173-184. Wiley Online Library, 2020.

[30] **NGUYEN, H. H. et al.** FakeSpotter: A simple but robust baseline for spotting AI-synthesized fake faces. In: *Proceedings of the 29th International Joint Conference on Artificial Intelligence*. 2020. p. 3444-3451.

[31] **P√âREZ-GARC√çA, A. et al.** Data augmentation techniques in CNNs using functional transformation. *Applied Sciences*, v. 8, n. 10, p. 1692, 2018.

[32] **PESSA, A. A. B.; RIBEIRO, H. V.** ordpy: A Python package for data analysis with permutation entropy and ordinal network methods. *Chaos*, v. 31, n. 6, p. 063110, 2021.

[33] **RIBEIRO, H. V. et al.** Complexity-Entropy Causality Plane as a Complexity Measure for Two-Dimensional Patterns. *PLOS ONE*, v. 7, p. e40689, 2012.

[34] **ROSSLER, A. et al.** FaceForensics++: Learning to detect manipulated facial images. In: *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2019. p. 1-11.

[35] **SMITH, J.; DOE, A.** Deep learning approaches for digital forensics: A comprehensive survey. *ACM Computing Surveys*, v. 54, n. 3, p. 1-37, 2021.

[36] **TAN, M.; LE, Q.** EfficientNet: Rethinking model scaling for convolutional neural networks. In: *International Conference on Machine Learning*. PMLR, 2019. p. 6105-6114.

[37] **THIES, J. et al.** Face2Face: Real-time face capture and reenactment of RGB videos. In: *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2016. p. 2387-2395.

[38] **VASWANI, A. et al.** Attention is all you need. In: *Advances in Neural Information Processing Systems*, v. 30, 2017.

[39] **WANG, K. et al.** Detecting both machine and human created fake face images. In: *Proceedings of the 2nd International Conference on Multimedia Information Processing and Retrieval*. 2019. p. 229-234.

[40] **YANG, X. et al.** Exposing deep fakes using inconsistent head poses. In: *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2019. p. 8261-8265.

[41] **YU, N. et al.** The eyes tell all: detecting fake face images via analyzing eye movements. *IEEE Transactions on Information Forensics and Security*, v. 16, p. 3443-3456, 2021.

[42] **ZHANG, X. et al.** Detecting fake images using DCT coefficient analysis. *Signal Processing*, v. 145, p. 98-110, 2018.

---

## üì¨ **Contato**

üì© **E-mail:** [fl@ic.ufal.br](mailto:fl@ic.ufal.br)  
üîó **LinkedIn:** [linkedin.com/in/fabio-linhares](https://www.linkedin.com/in/fabio-linhares)  
üêô **GitHub:** [github.com/fabio-linhares](https://github.com/fabio-linhares)  
üåê **Site do Projeto:** [fabiolinhares.com.br/ufal/orientacao/preprojeto](https://www.fabiolinhares.com.br/ufal/orientacao/preprojeto/preprojeto.html)

---

**Trabalho de Mestrado - Programa de P√≥s-Gradua√ß√£o em Inform√°tica**  
**Universidade Federal de Alagoas (UFAL)**  
**Orientador:** Prof. Dr. [Nome do Orientador]  
**Ano:** 2024
