
@article{ WOS:001526740300003,
Author = {Lan, Qiusong and Yang, Chengfu and A, Qinhua and Zhao, Jianlong and Jin,
   Li},
Title = {Joint spatial-frequency deepfake detection network based on dual-domain
   attention-enhanced deformable convolution},
Journal = {APPLIED INTELLIGENCE},
Year = {2025},
Volume = {55},
Number = {12},
Month = {JUL 12},
Abstract = {This paper proposes a novel Spatio-Frequency Deepfake Detection Network
   based on Dual-Domain Attention and Deformable Convolution (HFDCDNet).
   The core of the network is the HFDCD module, which simultaneously
   extracts spatial domain features and high-frequency information from the
   input feature maps. These two types of features are then fused using a
   specially designed bidirectional cross-attention mechanism.
   Specifically, the spatial features are extracted using a Deformable
   Convolution and Dual Attention-based module (DCD), which leverages
   deformable convolutions (DCN) and a spatial-channel attention mechanism
   to capture more comprehensive spatial representations. Meanwhile,
   high-frequency features are extracted using the High-Frequency
   Extraction (HFE) module, which learns frequency domain representations
   through high-pass filtering and frequency-aware convolutional learning.
   The bidirectional cross-attention mechanism facilitates complementary
   fusion and mutual enhancement of spatial and frequency features,
   enabling more fine-grained and holistic feature learning and improving
   detection performance. To evaluate the effectiveness of the proposed
   HFDCDNet, extensive experiments were conducted on two widely used public
   datasets: FaceForensics++ and Celeb-DF (V2). The results demonstrate
   that HFDCDNet achieves an accuracy (ACC) of 98.31\% and an area under
   the curve (AUC) of 99.51\% on the FaceForensics++ dataset, and 98.29\%
   ACC and 99.13\% AUC on the Celeb-DF (V2) dataset, outperforming many
   state-of-the-art methods. These results confirm that the proposed DCD
   module significantly enhances the model's ability to detect manipulated
   facial content.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Yang, CF (Corresponding Author), Yunnan Normal Univ, Sch Informat, Juxian St, Kunming 650500, Yunnan, Peoples R China.
   Yang, CF (Corresponding Author), Engn Res Ctr Comp Vis \& Intelligent Control Techno, Yunnan Prov Dept Educ, Juxian St, Kunming 650500, Yunnan, Peoples R China.
   Lan, Qiusong; Yang, Chengfu; A, Qinhua; Zhao, Jianlong; Jin, Li, Yunnan Normal Univ, Sch Informat, Juxian St, Kunming 650500, Yunnan, Peoples R China.
   Yang, Chengfu, Engn Res Ctr Comp Vis \& Intelligent Control Techno, Yunnan Prov Dept Educ, Juxian St, Kunming 650500, Yunnan, Peoples R China.},
DOI = {10.1007/s10489-025-06761-2},
Article-Number = {866},
ISSN = {0924-669X},
EISSN = {1573-7497},
Keywords = {Deepfake; DeformableConv; Attention; Frequency},
Keywords-Plus = {FACE MANIPULATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {704720234@qq.com
   yangchengfu@ynnu.edu.cn
   aqinhua@ynnu.edu.cn
   1362784075@qq.com
   2649120532@qq.com},
Affiliations = {Yunnan Normal University},
Funding-Acknowledgement = {Basic research project of natural science of Yunnan province
   {[}202301AT070065]; Basic research project of education department of
   Yunnan province {[}2023J0208, 2023J0209]},
Funding-Text = {This work was supported in part by the basic research project of natural
   science of Yunnan province under Grant 202301AT070065, in part by the
   basic research project of education department of Yunnan province under
   Grant 2023J0208 and 2023J0209.},
Cited-References = {Afchar D, 2018, IEEE INT WORKS INFOR.
   Brock A, 2019, Arxiv, DOI {[}arXiv:1809.11096, 10.48550/arXiv.1809.11096].
   Cao JY, 2022, PROC CVPR IEEE, P4103, DOI 10.1109/CVPR52688.2022.00408.
   Chen H.-S., 2021, P IEEE INT C MULT EX, P1.
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081.
   Chen ZH, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1985, DOI 10.1109/ICASSP39728.2021.9414225.
   Choi DH, 2020, IEEE IMAGE PROC, P823, DOI {[}10.1109/icip40778.2020.9190655, 10.1109/ICIP40778.2020.9190655].
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89.
   Das S, 2023, INT CONF BIOMETR THE, DOI 10.1109/IJCB57857.2023.10449301.
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929.
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672.
   Guo ZQ, 2023, EXPERT SYST APPL, V215, DOI 10.1016/j.eswa.2022.119361.
   Guo ZQ, 2023, COMPUT VIS IMAGE UND, V226, DOI 10.1016/j.cviu.2022.103587.
   Guo ZQ, 2021, COMPUT VIS IMAGE UND, V204, DOI 10.1016/j.cviu.2021.103170.
   Nguyen HH, 2019, Arxiv, DOI arXiv:1910.12467.
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453.
   Ke JP, 2023, NEURAL NETWORKS, V160, P216, DOI 10.1016/j.neunet.2023.01.001.
   Khormali A, 2024, IEEE ACCESS, V12, P58114, DOI 10.1109/ACCESS.2024.3392512.
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327.
   Lin H, 2023, DIGIT SIGNAL PROCESS, V134, DOI 10.1016/j.dsp.2022.103895.
   Mirza M, 2014, Arxiv, DOI arXiv:1411.1784.
   Nirkin Y, 2022, IEEE T PATTERN ANAL, V44, P6111, DOI 10.1109/TPAMI.2021.3093446.
   Thao PNM, 2024, PROCEEDINGS OF THE 33RD ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2024, P3974, DOI 10.1145/3627673.3679962.
   Qian YY, 2020, Img Proc Comp Vis Re, V12357, P86, DOI 10.1007/978-3-030-58610-2\_6.
   Radford A, 2016, Arxiv, DOI arXiv:1511.06434.
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009.
   Shang ZH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107950.
   Sun K, 2022, LECT NOTES COMPUT SC, V13674, P111, DOI 10.1007/978-3-031-19781-9\_7.
   Tan CC, 2024, AAAI CONF ARTIF INTE, P5052.
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035.
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262.
   Tolosana R, 2020, INFORM FUSION, V64, P131, DOI 10.1016/j.inffus.2020.06.014.
   Wang JK, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P615, DOI 10.1145/3512527.3531415.
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2\_1.
   Xu K, 2023, MULTIMED TOOLS APPL, V82, P29501, DOI 10.1007/s11042-023-14626-4.
   Yadav A, 2024, ENG APPL ARTIF INTEL, V127, DOI 10.1016/j.engappai.2023.107443.
   Yadav A, 2023, EXPERT SYST APPL, V232, DOI 10.1016/j.eswa.2023.120898.
   Yang GM, 2023, MULTIMEDIA SYST, V29, P2399, DOI 10.1007/s00530-023-01118-6.
   Yang ZM, 2023, IEEE T INF FOREN SEC, V18, P1696, DOI 10.1109/TIFS.2023.3249566.
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222.
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319.},
Number-of-Cited-References = {42},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Appl. Intell.},
Doc-Delivery-Number = {4UD7P},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001526740300003},
DA = {2025-08-25},
}
