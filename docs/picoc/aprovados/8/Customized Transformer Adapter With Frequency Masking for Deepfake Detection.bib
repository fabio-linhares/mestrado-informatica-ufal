
@article{ WOS:001511714600007,
Author = {Shi, Zenan and Chen, Haipeng and Jia, Yixin and Zhang, Dong and Lu, Wei
   and Yang, Xun},
Title = {Customized Transformer Adapter With Frequency Masking for Deepfake
   Detection},
Journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
Year = {2025},
Volume = {20},
Pages = {5904-5918},
Abstract = {The rapid advancement of AI-generated content has intensified concerns
   over deepfakes due to increasingly sophisticated and visually convincing
   forgeries. To this end, the pre-trained Vision Transformer (ViT) model
   has become a de facto choice for deepfake detection, thanks to its
   powerful learning capability. Despite favorable results achieved by
   existing ViT-based methods, they have inherent limitations that could
   result in suboptimal performance in scenarios with continuously evolving
   forgery techniques, such as overfitting to single forgery patterns or
   placing excessive emphasis on dominant forgery regions. In this paper,
   we propose CUTA, a simple yet effective deepfake detection paradigm that
   utilizes ViT adapters as the medium and fully exploits the spatial- and
   frequency-domain features of given images to overcome the limitations of
   existing methods. Specifically, CUTA focuses on frequency domain masking
   within the input space, which obscures parts of the high-frequency image
   to intensify the training challenge while preserving subtle forgery cues
   in the frequency domain to facilitate comprehensive forgery
   representations. Furthermore, we propose two task-customized modules
   within the ViT model, i.e., the texture enhancement module and the
   multi-scale perceptron module, to seamlessly integrate local texture and
   rich contextual features. These two modules ensure an organic
   interaction between the task-specific forgery patterns and general
   semantic features within the pre-trained ViT framework. The experimental
   results on several publicly available benchmarks demonstrate CUTA's
   superiority in performance, particularly showcasing its significant
   advantages in both cross-dataset and cross-manipulation scenarios. Code
   and models are available at https://github.com/Zenanshi92/CUTA},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Zhang, D (Corresponding Author), Hong Kong Univ Sci \& Technol, Dept Elect \& Comp Engn, Hong Kong, Peoples R China.
   Shi, Zenan; Chen, Haipeng, Jilin Univ, Coll Comp Sci \& Technol, Key Lab Symbol Computat \& Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
   Jia, Yixin, Jilin Univ, Coll Software, Key Lab Symbol Computat \& Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
   Zhang, Dong, Hong Kong Univ Sci \& Technol, Dept Elect \& Comp Engn, Hong Kong, Peoples R China.
   Lu, Wei, Sun Yat Sen Univ, Sch Comp Sci \& Engn, Minist Educ, Guangdong Prov Key Lab Informat Secur Technol,Key, Guangzhou 510006, Peoples R China.
   Yang, Xun, Univ Sci \& Technol China, Sch Informat Sci \& Technol, Hefei 230026, Peoples R China.},
DOI = {10.1109/TIFS.2025.3574983},
ISSN = {1556-6013},
EISSN = {1556-6021},
Keywords = {Forgery; Deepfakes; Semantics; Faces; Frequency-domain analysis; Feature
   extraction; Training; Transformers; Adaptation models; Visualization;
   Deepfake detection; vision transformer; ViT adapter; frequency domain
   masking},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Author-Email = {shizn@jlu.edu.cn
   chenhp@jlu.edu.cn
   yxjia23@mails.jlu.edu.cn
   dongz@ust.hk
   luwei3@mail.sysu.edu.cn
   xyang21@ustc.edu.cn},
Affiliations = {Jilin University; Jilin University; Hong Kong University of Science \&
   Technology; Sun Yat Sen University; Chinese Academy of Sciences;
   University of Science \& Technology of China, CAS},
Funding-Acknowledgement = {Key Projects of Science and Technology Development Plan of Jilin
   Province {[}20230201088GX]; National Natural Science Foundation of China
   {[}62276112, 62441237]},
Funding-Text = {This work was supported in part by the Key Projects of Science and
   Technology Development Plan of Jilin Province under Grant 20230201088GX
   and in part by the National Natural Science Foundation of China under
   Grant 62276112 and Grant 62441237.},
Cited-References = {{[}Anonymous], 2024, Video Generation Models as World Simulators.
   Ba ZJ, 2024, AAAI CONF ARTIF INTE, P719.
   Bai WM, 2023, PROC CVPR IEEE, P24709, DOI 10.1109/CVPR52729.2023.02367.
   Cao JY, 2024, INT J COMPUT VISION, V132, P5862, DOI 10.1007/s11263-024-02151-2.
   Cao JY, 2022, PROC CVPR IEEE, P4103, DOI 10.1109/CVPR52688.2022.00408.
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184.
   Chen RW, 2020, MM `20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630.
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081.
   Chen Z., 2023, PROC 11 INT C LEARN.
   Chen ZX, 2024, Arxiv, DOI arXiv:2403.18471.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Deng JK, 2019, Arxiv, DOI arXiv:1905.00641.
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397.
   Doloriel CT, 2024, INT CONF ACOUST SPEE, P13466, DOI 10.1109/ICASSP48485.2024.10446290.
   Dong SC, 2023, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR52729.2023.00389.
   Dosovitskiy Alexey., 2021, PROC INT C LEARN REP, P2021, DOI DOI 10.48550/ARXIV.2010.11929.
   Feichtenhofer C, 2022, ADV NEUR IN.
   Gao CL, 2024, INT CONF ACOUST SPEE, P4690, DOI 10.1109/ICASSP48485.2024.10446543.
   Gao P, 2022, ADV NEUR IN.
   Gu QQ, 2022, AAAI CONF ARTIF INTE, P735.
   Gu ZH, 2022, AAAI CONF ARTIF INTE, P744.
   Guan WN, 2024, IEEE T INF FOREN SEC, V19, P5345, DOI 10.1109/TIFS.2024.3396064.
   Guo MH, 2023, IEEE T PATTERN ANAL, V45, P5436, DOI 10.1109/TPAMI.2022.3211006.
   Guo Y, 2023, IEEE I CONF COMP VIS, P20761, DOI 10.1109/ICCV51070.2023.01903.
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500.
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553.
   Houlsby N, 2019, PR MACH LEARN RES, V97.
   Hu J, 2023, Arxiv, DOI arXiv:2303.01740.
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4\_41.
   Kingma DP., 2014, PREPRINT.
   Kong CQ, 2024, Arxiv, DOI arXiv:2404.08452.
   Li D, 2018, AAAI CONF ARTIF INTE, P3490.
   Li JY, 2023, PROC CVPR IEEE, P11578, DOI 10.1109/CVPR52729.2023.01114.
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505.
   Li YH, 2022, LECT NOTES COMPUT SC, V13669, P280, DOI 10.1007/978-3-031-20077-9\_17.
   Li YZ, 2018, IEEE INT WORKS INFOR.
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327.
   Liu DC, 2023, IEEE T INF FOREN SEC, V18, P4272, DOI 10.1109/TIFS.2023.3293951.
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083.
   Liu JW, 2024, IEEE T INF FOREN SEC, V19, P1922, DOI 10.1109/TIFS.2023.3344293.
   Liu ZZ, 2020, PROC CVPR IEEE, P8057, DOI 10.1109/CVPR42600.2020.00808.
   Luo AW, 2025, IEEE T CIRC SYST VID, V35, P4116, DOI {[}10.1109/tcsvt.2024.3522091, 10.1109/TCSVT.2024.3522091].
   Luo AW, 2024, IEEE T INF FOREN SEC, V19, P1168, DOI 10.1109/TIFS.2023.3332218.
   Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605.
   Meng Y, 2022, ADV NEUR IN.
   Miao CT, 2023, IEEE T INF FOREN SEC, V18, P1039, DOI 10.1109/TIFS.2022.3233774.
   Nirkin Y, 2023, IEEE T PATTERN ANAL, V45, P560, DOI 10.1109/TPAMI.2022.3155571.
   Ojha U, 2023, PROC CVPR IEEE, P24480, DOI 10.1109/CVPR52729.2023.02345.
   Peng CL, 2024, IEEE T INF FOREN SEC, V19, P4507, DOI 10.1109/TIFS.2024.3381823.
   Prajwal KR, 2020, MM `20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532.
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009.
   Rosenfeld A, 2020, IEEE T PATTERN ANAL, V42, P651, DOI 10.1109/TPAMI.2018.2884462.
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
   Shao R., 2023, P INT J COMP VIS JAN, P1.
   Shi ZN, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P1387.
   Shi ZA, 2023, IEEE T CIRC SYST VID, V33, P4907, DOI 10.1109/TCSVT.2023.3251444.
   Shuai C, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P7131, DOI 10.1145/3581783.3612386.
   Sun K, 2022, LECT NOTES COMPUT SC, V13674, P111, DOI 10.1007/978-3-031-19781-9\_7.
   Sun K, 2022, AAAI CONF ARTIF INTE, P2316.
   Sun K, 2021, AAAI CONF ARTIF INTE, V35, P2638.
   Sun ZM, 2023, IEEE I CONF COMP VIS, P20825, DOI 10.1109/ICCV51070.2023.01909.
   Tan CC, 2024, AAAI CONF ARTIF INTE, P5052.
   Tan CC, 2023, PROC CVPR IEEE, P12105, DOI 10.1109/CVPR52729.2023.01165.
   Tan LF, 2023, AAAI CONF ARTIF INTE, P5276.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262.
   Tian JH, 2024, IEEE T INF FOREN SEC, V19, P3814, DOI 10.1109/TIFS.2024.3372773.
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579.
   Vaswani A, 2017, ADV NEUR IN, V30.
   Verdoliva L, 2020, IEEE J-STSP, V14, P910, DOI 10.1109/JSTSP.2020.3002101.
   Wang CR, 2021, PROC CVPR IEEE, P14918, DOI 10.1109/CVPR46437.2021.01468.
   Wang HQ, 2023, PROC CVPR IEEE, P2122, DOI 10.1109/CVPR52729.2023.00211.
   Wang JK, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P615, DOI 10.1145/3512527.3531415.
   Wang TY, 2023, AAAI CONF ARTIF INTE, P14548.
   Wang Y, 2023, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR52729.2023.00703.
   Wu YQ, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P1759, DOI 10.1145/3581783.3612102.
   Xie J., 2022, P 11 INT C LEARN REP, P1.
   Xu YT, 2021, 2021 INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2021), DOI 10.1109/IJCB52358.2021.9484407.
   Yan ZY, 2023, IEEE I CONF COMP VIS, P22355, DOI 10.1109/ICCV51070.2023.02048.
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2\_6.
   Zhang BG, 2022, AAAI CONF ARTIF INTE, P3243.
   Zhang D., 2020, Adv. Neural Inf. Process. Syst., V33, P655.
   Zhang DC, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5833, DOI 10.1145/3503161.3547913.
   Zhang D, 2025, Arxiv, DOI {[}arXiv:2401.13174, arXiv:2401.13174].
   Zhang D, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2380, DOI 10.1145/3503161.3547858.
   Zhang D, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108594.
   Zhang D, 2020, Img Proc Comp Vis Re, V12373, P323, DOI 10.1007/978-3-030-58604-1\_20.
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222.
   Zhao WL, 2023, PROC CVPR IEEE, P8568, DOI 10.1109/CVPR52729.2023.00828.
   Zhu XY, 2023, IEEE T PATTERN ANAL, V45, P8342, DOI 10.1109/TPAMI.2022.3233586.
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555.
   Zhuang WY, 2022, LECT NOTES COMPUT SC, V13665, P391, DOI 10.1007/978-3-031-20065-6\_23.
   Zi BJ, 2020, MM `20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769.},
Number-of-Cited-References = {93},
Times-Cited = {0},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {4},
Journal-ISO = {IEEE Trans. Inf. Forensic Secur.},
Doc-Delivery-Number = {3YA3T},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001511714600007},
DA = {2025-08-25},
}
